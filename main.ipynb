{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = \"/Users/aagamchhajer/Desktop/aagam-projects/DeepSeek-RAG/pdf_dir\"\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir = input_dir_path,\n",
    "    required_exts = [\".pdf\"],\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup llm & embedding model\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding( model_name=\"Snowflake/snowflake-arctic-embed-m\", trust_remote_code=True)\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=Ollama(model=\"deepseek-r1:8b\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 3/3 [00:00<00:00, 3036.42it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating an index over loaded data\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "\n",
    "# Create the query engine, where we use a cohere reranker on the fetched nodes\n",
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# ====== Customise prompt template ======\n",
    "qa_prompt_tmpl_str = (\n",
    "\"Context information is below.\\n\"\n",
    "\"---------------------\\n\"\n",
    "\"{context_str}\\n\"\n",
    "\"---------------------\\n\"\n",
    "\"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "\"Query: {query_str}\\n\"\n",
    "\"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "# Generate the response\n",
    "response = query_engine.query(\"Define a LLM Architecture?\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to define an LLM architecture based on the context provided. Let me start by understanding what's given.\n",
      "\n",
      "From page_label 2, a basic LLM app has four main elements: Foundation LLM, Vector database, Orchestration framework, and UI framework. The context also mentions that when moving to a professional level, an advanced architecture with more components is needed.\n",
      "\n",
      "On page_label 3, the advanced version includes ten elements: Foundation LLM, Vector database, Orchestration framework, UI framework, Backend framework, Integrated external APIs, Validation framework, LLMOps, LLM cache, and Cloud provider.\n",
      "\n",
      "So, for a basic definition, I should probably mention both basic and advanced architectures. The basic has four parts, while the advanced adds six more components on top of the original four.\n",
      "\n",
      "I need to make sure I clearly differentiate between the two levels, highlighting each component without getting too technical. Maybe start by defining an LLM architecture in general, then break it down into basic and advanced.\n",
      "\n",
      "Wait, but should I include specifics like which frameworks or tools are used? Probably not, since the context doesn't provide that detail. It's more about the components than the specific implementations.\n",
      "\n",
      "I also need to ensure clarity and conciseness, making sure each point is explained simply without jargon where possible.\n",
      "</think>\n",
      "\n",
      "An LLM (Large Language Model) architecture refers to the structural components that make up a system utilizing large language models. This architecture can be categorized into basic and advanced levels:\n",
      "\n",
      "**Basic LLM Architecture:**\n",
      "Comprises four core elements:\n",
      "1. **Foundation LLM**: The foundational model, such as ChatGPT, which serves as the basis for building applications.\n",
      "2. **Vector Database**: Stores private data in vector form.\n",
      "3. **Orchestration Framework**: A \"language\" used to coordinate various parts of the application.\n",
      "4. **UI Framework**: The user interface that acts as the visual front end.\n",
      "\n",
      "**Advanced LLM Architecture:**\n",
      "Builds upon the basic structure with additional components:\n",
      "5. **Backend Framework**: Manages server-side logic and data processing.\n",
      "6. **Integrated External APIs**: Incorporates external services for enhanced functionality.\n",
      "7. **Validation Framework**: Ensures content quality through checks.\n",
      "8. **LLMOps**: Facilitates moving the application to production, including deployment and monitoring.\n",
      "9. **LLM Cache**: Stores previous responses to improve efficiency by avoiding repeated searches.\n",
      "10. **Cloud Provider**: Manages application storage on cloud platforms.\n",
      "\n",
      "In essence, an LLM architecture is a modular framework that can evolve from a simple setup to a more complex, scalable system with additional functionalities and layers as needs grow.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
